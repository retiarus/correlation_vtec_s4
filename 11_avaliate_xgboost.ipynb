{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from evolutionary_search import EvolutionaryAlgorithmSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import recall_score, make_scorer, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "from utils import MySet\n",
    "\n",
    "from utils import local_data\n",
    "from utils import window\n",
    "from utils import Scale, give_error\n",
    "from utils import generate_and_avaliate_model\n",
    "\n",
    "from utils import location_station, find_set_sunrise, find_set_sunset\n",
    "\n",
    "#%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "latter_size = 14\n",
    "plt.rcParams['legend.fontsize'] = latter_size \n",
    "plt.rcParams['font.size'] = latter_size \n",
    "plt.rcParams['axes.labelsize'] = latter_size\n",
    "plt.rcParams['xtick.labelsize'] = latter_size\n",
    "plt.rcParams['ytick.labelsize'] = latter_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/sj2_analise_update2_drop.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vtec', 'vtec_dt', 'vtec_dt2', 'gvtec1', 'gvtec1_dt', 'gvtec2',\n",
       "       'gvtec2_dt', 'gvtec3', 'gvtec3_dt', 's4', 'state_night', 'state_dawn',\n",
       "       'vm1', 'vd1', 'vm2', 'vd2', 'gvtec1_dt_lag_9', 'gvtec2_dt_lag_20',\n",
       "       'vtec_dt_lag_3', 'vtec_i/vtec_i-1', 'roti_3', 'roti_5', 'roti_7',\n",
       "       'roti_9', 'roti_11', 'roti_13', 'gvtec1/gvtec2', 'gvtec1_dt/gvtec2_dt',\n",
       "       'doy', 'ut', 'discretize_s4', 'discretize_s4_02', 'discretize_s4_03',\n",
       "       'discretize_s4_04', 'discretize_s4_05', 'discretize_s4_06',\n",
       "       'discretize_s4_07'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = MySet('original', ['vtec', 'vtec_dt', 'vtec_dt2', 'gvtec1', 'gvtec1_dt', 'gvtec2',\n",
    "       'gvtec2_dt', 'gvtec3', 'gvtec3_dt', 'state_night', 'state_dawn', 'vm1', 'vd1', 'vm2', 'vd2', 'gvtec1_dt_lag_9',\n",
    "       'gvtec2_dt_lag_20', 'vtec_dt_lag_3', 'vtec_i/vtec_i-1', 'roti_3',\n",
    "       'roti_5', 'roti_7', 'roti_9', 'roti_11', 'roti_13', 'gvtec1/gvtec2',\n",
    "       'gvtec1_dt/gvtec2_dt', 'doy', 'ut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4777e+03 1.6600e+01 7.0000e-01 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [6.0800e+01 8.2800e+01 1.3000e+00 1.0000e-01 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [1.1500e+01 1.4300e+01 1.3600e+01 6.0000e-01 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [1.6000e+00 1.0000e+00 2.4000e+00 4.9000e+00 1.0000e-01 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [0.0000e+00 8.0000e-01 1.0000e-01 1.0000e-01 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00]]\n",
      "('roti_11', 0.021871204)\n",
      "('roti_5', 0.038072094)\n",
      "('gvtec3', 0.051032808)\n",
      "('gvtec1_dt_lag_9', 0.04981774)\n",
      "('gvtec1_dt/gvtec2_dt', 0.009315512)\n",
      "('vtec_dt2', 0.022681247)\n",
      "('gvtec1', 0.02146618)\n",
      "('vm2', 0.028756581)\n",
      "('gvtec2_dt_lag_20', 0.07614419)\n",
      "('state_dawn', 0.021871204)\n",
      "('gvtec2', 0.05508303)\n",
      "('gvtec3_dt', 0.05184285)\n",
      "('vtec_i/vtec_i-1', 0.019036047)\n",
      "('doy', 0.07857432)\n",
      "('vm1', 0.03199676)\n",
      "('vtec_dt', 0.0076954234)\n",
      "('vtec', 0.07492912)\n",
      "('vd2', 0.03928716)\n",
      "('gvtec2_dt', 0.052652895)\n",
      "('vd1', 0.036452007)\n",
      "('roti_13', 0.01782098)\n",
      "('vtec_dt_lag_3', 0.010530579)\n",
      "('ut', 0.080599435)\n",
      "('roti_9', 0.019846091)\n",
      "('gvtec1/gvtec2', 0.021871204)\n",
      "('roti_7', 0.02308627)\n",
      "('gvtec1_dt', 0.027946537)\n",
      "('roti_3', 0.009720535)\n",
      "('state_night', 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ut', 0.080599435),\n",
       " ('doy', 0.07857432),\n",
       " ('gvtec2_dt_lag_20', 0.07614419),\n",
       " ('vtec', 0.07492912),\n",
       " ('gvtec2', 0.05508303),\n",
       " ('gvtec2_dt', 0.052652895),\n",
       " ('gvtec3_dt', 0.05184285),\n",
       " ('gvtec3', 0.051032808),\n",
       " ('gvtec1_dt_lag_9', 0.04981774),\n",
       " ('vd2', 0.03928716),\n",
       " ('roti_5', 0.038072094),\n",
       " ('vd1', 0.036452007),\n",
       " ('vm1', 0.03199676),\n",
       " ('vm2', 0.028756581),\n",
       " ('gvtec1_dt', 0.027946537),\n",
       " ('roti_7', 0.02308627),\n",
       " ('vtec_dt2', 0.022681247),\n",
       " ('roti_11', 0.021871204),\n",
       " ('state_dawn', 0.021871204),\n",
       " ('gvtec1/gvtec2', 0.021871204),\n",
       " ('gvtec1', 0.02146618),\n",
       " ('roti_9', 0.019846091),\n",
       " ('vtec_i/vtec_i-1', 0.019036047),\n",
       " ('roti_13', 0.01782098),\n",
       " ('vtec_dt_lag_3', 0.010530579),\n",
       " ('roti_3', 0.009720535),\n",
       " ('gvtec1_dt/gvtec2_dt', 0.009315512),\n",
       " ('vtec_dt', 0.0076954234),\n",
       " ('state_night', 0.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select data\n",
    "instance_set = list(original.set)\n",
    "X = df[instance_set].values\n",
    "y = df['discretize_s4'].values\n",
    "\n",
    "recall_inbalanced_score = make_scorer(recall_score, average='macro')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "       \n",
    "# suffle the train data\n",
    "order = np.random.permutation(len(X_train))\n",
    "X_train = np.array([X_train[i] for i in order])\n",
    "y_train = np.array([y_train[i] for i in order])\n",
    "\n",
    "mod = XGBClassifier(num_class=7, objective=\"multi:softmax\", metric=\"mlogloss\")\n",
    "# implement Kfold cross validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "errors = []\n",
    "conf_matrix = np.zeros((7, 7))\n",
    "for train_index, test_index in kf.split(X_train, y_train):\n",
    "    # generate standardize transformation for (x,y)\n",
    "    X_scaler = StandardScaler() # transformation for X\n",
    "    X_scaler.fit(X_train[train_index])\n",
    "        \n",
    "    mod.fit(X_scaler.transform(X_train[train_index]),\n",
    "            y_train[train_index])\n",
    "    \n",
    "    # use the final model to avaliate the error in a sample of the time series\n",
    "    X_validate = X_scaler.transform(X_test)\n",
    "    \n",
    "    predito = mod.predict(X_validate)\n",
    "    conf_matrix += confusion_matrix(y_test, predito, labels=[0, 1, 2, 3, 4, 5, 6] )\n",
    "\n",
    "conf_matrix /= 10.0\n",
    "print(conf_matrix)\n",
    "\n",
    "list_values = []\n",
    "for feature, feat_importance in zip(instance_set, mod.feature_importances_):\n",
    "    list_values.append((feature, feat_importance))\n",
    "    print((feature, feat_importance))\n",
    "    \n",
    "def get_second(value):\n",
    "    return value[1]\n",
    "\n",
    "list_values.sort(key=get_second, reverse=True)\n",
    "\n",
    "list_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "b'[19:39:19] src/objective/hinge.cu:50: Check failed: preds.Size() == info.labels_.Size() (7098 vs. 3549) labels are not correctly providedpreds.size=7098, label.size=3549\\n\\nStack trace returned 10 entries:\\n[bt] (0) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(dmlc::StackTrace()+0x42) [0x7f988c066092]\\n[bt] (1) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x18) [0x7f988c066698]\\n[bt] (2) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(xgboost::obj::HingeObj::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3d1) [0x7f988c0e9d11]\\n[bt] (3) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x362) [0x7f988c073be2]\\n[bt] (4) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(XGBoosterUpdateOneIter+0x35) [0x7f988c1c79d5]\\n[bt] (5) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f98c4927b10]\\n[bt] (6) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x47e) [0x7f98c49274df]\\n[bt] (7) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x283) [0x7f98c4b3b7c3]\\n[bt] (8) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x8e6f) [0x7f98c4b32e6f]\\n[bt] (9) /var/lib/jupyterhub/anaconda/envs/dscience/bin/../lib/libpython3.6m.so.1.0(_PyObject_FastCallDict+0x8b) [0x7f98cba9fa0b]\\n\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-489b15ac7b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     mod.fit(X_scaler.transform(X_train[train_index]),\n\u001b[0;32m---> 31\u001b[0;31m             y_train[train_index])\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# use the final model to avaliate the error in a sample of the time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    698\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1045\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \"\"\"\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: b'[19:39:19] src/objective/hinge.cu:50: Check failed: preds.Size() == info.labels_.Size() (7098 vs. 3549) labels are not correctly providedpreds.size=7098, label.size=3549\\n\\nStack trace returned 10 entries:\\n[bt] (0) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(dmlc::StackTrace()+0x42) [0x7f988c066092]\\n[bt] (1) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x18) [0x7f988c066698]\\n[bt] (2) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(xgboost::obj::HingeObj::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3d1) [0x7f988c0e9d11]\\n[bt] (3) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x362) [0x7f988c073be2]\\n[bt] (4) /var/lib/jupyterhub/anaconda/envs/dscience/lib/libxgboost.so(XGBoosterUpdateOneIter+0x35) [0x7f988c1c79d5]\\n[bt] (5) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call_unix64+0x4c) [0x7f98c4927b10]\\n[bt] (6) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/../../libffi.so.6(ffi_call+0x47e) [0x7f98c49274df]\\n[bt] (7) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(_ctypes_callproc+0x283) [0x7f98c4b3b7c3]\\n[bt] (8) /var/lib/jupyterhub/anaconda/envs/dscience/lib/python3.6/lib-dynload/_ctypes.cpython-36m-x86_64-linux-gnu.so(+0x8e6f) [0x7f98c4b32e6f]\\n[bt] (9) /var/lib/jupyterhub/anaconda/envs/dscience/bin/../lib/libpython3.6m.so.1.0(_PyObject_FastCallDict+0x8b) [0x7f98cba9fa0b]\\n\\n'"
     ]
    }
   ],
   "source": [
    "# select data\n",
    "instance_set = list(original.set)\n",
    "X = df[instance_set].values\n",
    "y = df['discretize_s4_02'].values\n",
    "\n",
    "recall_inbalanced_score = make_scorer(recall_score, average='macro')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "       \n",
    "# suffle the train data\n",
    "order = np.random.permutation(len(X_train))\n",
    "X_train = np.array([X_train[i] for i in order])\n",
    "y_train = np.array([y_train[i] for i in order])\n",
    "\n",
    "mod = XGBClassifier(num_class=2, objective=\"binary:hinge\", metric=\"logloss\")\n",
    "# implement Kfold cross validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "errors = []\n",
    "conf_matrix = np.zeros((2, 2))\n",
    "for train_index, test_index in kf.split(X_train, y_train):\n",
    "    # generate standardize transformation for (x,y)\n",
    "    X_scaler = StandardScaler() # transformation for X\n",
    "    X_scaler.fit(X_train[train_index])\n",
    "        \n",
    "    mod.fit(X_scaler.transform(X_train[train_index]),\n",
    "            y_train[train_index])\n",
    "    \n",
    "    # use the final model to avaliate the error in a sample of the time series\n",
    "    X_validate = X_scaler.transform(X_test)\n",
    "    \n",
    "    predito = mod.predict(X_validate)\n",
    "    conf_matrix += confusion_matrix(y_test, predito, labels=[0, 1] )\n",
    "\n",
    "conf_matrix /= 10.0\n",
    "print(conf_matrix)\n",
    "\n",
    "list_values = []\n",
    "for feature, feat_importance in zip(instance_set, mod.feature_importances_):\n",
    "    list_values.append((feature, feat_importance))\n",
    "    print((feature, feat_importance))\n",
    "    \n",
    "def get_second(value):\n",
    "    return value[1]\n",
    "\n",
    "list_values.sort(key=get_second, reverse=True)\n",
    "\n",
    "list_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dscience)",
   "language": "python",
   "name": "dscience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
