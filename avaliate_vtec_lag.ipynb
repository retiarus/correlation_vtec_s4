{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import pdb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from utils import local_data\n",
    "from utils import window\n",
    "from utils import Scale, give_error\n",
    "from utils import generate_and_avaliate_model\n",
    "\n",
    "from utils import location_station, find_set_sunrise, find_set_sunset\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "latter_size = 14\n",
    "plt.rcParams['legend.fontsize'] = latter_size \n",
    "plt.rcParams['font.size'] = latter_size \n",
    "plt.rcParams['axes.labelsize'] = latter_size\n",
    "plt.rcParams['xtick.labelsize'] = latter_size\n",
    "plt.rcParams['ytick.labelsize'] = latter_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/df_vtec_lags.pkl')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag_0\n",
      "Fitting 10 folds for each of 4320 candidates, totalling 43200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 29.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 41.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 60.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 88.5min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 120.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 156.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4885 tasks      | elapsed: 197.2min\n",
      "[Parallel(n_jobs=-1)]: Done 5816 tasks      | elapsed: 240.6min\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestRegressor\n",
    "\n",
    "errors = []\n",
    "for i in range(0, 24):\n",
    "    instances_set = ['lag_'+str(i) for i in range(0,i+1)]\n",
    "    print('lag_'+str(i))\n",
    "\n",
    "    # select data\n",
    "    X = df[instances_set].values\n",
    "    y = df['s4'].values\n",
    "\n",
    "    size = len(X)\n",
    "    last_element = size - size//10\n",
    "\n",
    "    X_train = X[0:last_element]\n",
    "    y_train = y[0:last_element] \n",
    "    \n",
    "    # define parameters for grid_search\n",
    "    n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 180, num=11)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    param_grid = {'model__n_estimators': n_estimators,\n",
    "                  'model__max_features': max_features,\n",
    "                  'model__max_depth': max_depth,\n",
    "                  'model__min_samples_split': min_samples_split,\n",
    "                  'model__min_samples_leaf': min_samples_leaf,\n",
    "                  'model__bootstrap': bootstrap}\n",
    "       \n",
    "    estimators = []\n",
    "    estimators.append(('standardize', StandardScaler()))\n",
    "    estimators.append(('model', model()))\n",
    "    pipeline = Pipeline(estimators)\n",
    "    \n",
    "    clf = GridSearchCV(estimator=pipeline,\n",
    "                       param_grid=param_grid,\n",
    "                       cv=10,\n",
    "                       verbose=2,\n",
    "                       n_jobs=-1,\n",
    "                       scoring='neg_mean_squared_error')\n",
    "      \n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X_train)\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    \n",
    "    print(best_estimator_.get_params()['model'].get_params())\n",
    "    mod = model(**clf.best_estimator_.get_params()['model'].get_params())\n",
    "    mod.fit(X_train, y_train)\n",
    "    \n",
    "    # use the final model to avaliate the error in a sample of the time series\n",
    "    X_validate = X_scaler.transform(X[last_element:size+1])\n",
    "    y_validate = y[last_element:size+1]\n",
    "    \n",
    "    index = df.index.values[last_element:size+1]\n",
    "    df_aux = pd.DataFrame(index=index)\n",
    "    df_aux['predito'] = mod.predict(X_validate)\n",
    "    df_aux['real'] = y_validate\n",
    "\n",
    "    print('Error for the time series sample:')\n",
    "    dict_error = give_error(df_aux['real'].values, df_aux['predito'].values, cut_value=0.2);\n",
    "    dict_error['name'] = 'lag_'+str(i)\n",
    "    errors.append(dict_error)\n",
    "\n",
    "    # plot the time series predict against the real values\n",
    "    ax = df_aux.plot(figsize=(18, 8));\n",
    "    plt.xlabel('UT')\n",
    "\n",
    "    lat, long = location_station('sj2')\n",
    "    set_of_sunrise = find_set_sunrise(df_aux, lat, long)\n",
    "    set_of_sunset = find_set_sunset(df_aux, lat, long)\n",
    "    for i in set_of_sunrise:\n",
    "        ax.axvline(x=i, color='y')\n",
    "    for i in set_of_sunset[0:-1]:\n",
    "        ax.axvline(x=i, color='r')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(errors) \n",
    "df.index = df['name']\n",
    "del df['name']\n",
    "df = df[['tp', 'tn', 'fp', 'fn', 're', 'pod', 'far', 'acc', 'precission', 'recall', 'f1', 'kappa', 'me', 'tse', 'mse']]\n",
    "pd.set_option('precision', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dscience)",
   "language": "python",
   "name": "dscience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
